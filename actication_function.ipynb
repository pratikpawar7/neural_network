{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bc62a-a347-40ca-99f4-543e6bcc185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. What is an activation function in the context of artificial neural networks?\n",
    "'''\n",
    "\n",
    "An activation function in neural networks determines the output of a neuron by transforming the \n",
    "weighted sum of inputs into a non-linear output. \n",
    "It introduces non-linearity into the model, allowing the network to learn and model complex patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02160b8b-2ec8-4738-9dfe-452bc9acc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2. What are some common types of activation functions used in neural networks?\n",
    "'''\n",
    "\n",
    "Sigmoid: \n",
    "\n",
    "ReLU (Rectified Linear Unit): \n",
    "\n",
    "Leaky ReLU: \n",
    "\n",
    "Tanh (Hyperbolic Tangent): \n",
    "\n",
    "Softmax  activation function : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb4f56-9af4-4b69-a5cc-83c1f5649077",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "'''\n",
    "\n",
    "Activation functions impact the training process by introducing non-linearity, \n",
    "which enables the network to model complex relationships. \n",
    "They help create decision boundaries for classification problems and influence\n",
    "how the gradient-based optimization algorithms, like gradient descent, adjust the network's weights.\n",
    "Poor activation function choices can lead to issues like vanishing or exploding gradients, affecting network performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab1db58-828d-4e88-8aa7-5f30e2829b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "'''\n",
    "\n",
    "How it works: The sigmoid function maps any input into a value between 0 and 1, making it useful for binary classification. \n",
    "\n",
    " \n",
    "Advantages: It gives a smooth gradient and outputs values as probabilities, which is useful for binary\n",
    "                classification problems.\n",
    "\n",
    "Disadvantages: Sigmoid suffers from the vanishing gradient problem, where gradients become very small\n",
    "                for large positive or negative inputs, slowing down learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72120cb3-d7bb-4051-94ce-6c7ebff4c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5. What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "'''\n",
    "\n",
    "ReLU Function: \n",
    "\n",
    "f(x)=max(0,x)\n",
    "It outputs the input if it's positive; otherwise, it outputs zero.\n",
    "Differences from Sigmoid:\n",
    "Range: ReLU outputs values in the range [0, âˆž), whereas sigmoid outputs between (0, 1).\n",
    "Efficiency: ReLU is computationally simpler and allows faster training due to its non-saturating gradient, while sigmoid suffers from the vanishing gradient problem.\n",
    "Sparsity: ReLU creates sparse activations (many outputs are zero), which can improve efficiency and prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae1d17-b703-4768-b099-096e772dcf52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0937ea-8e7a-4e82-9864-d8c8bea47823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed86ff-b5fe-4f77-b3b0-e4a842945cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d0ce9-4d2a-434c-9119-d78cd4f81547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1002b1-eb9a-4779-937e-0bd521860941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1b612-f4f1-41b8-a721-0d0a73385a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41526a-5f85-4963-b437-2a3870b6f0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2238a5e-d1cf-4a81-96ed-fa2e21074c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b0fdc-e0a9-406e-9afd-77f5b57e0287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60368e-d7d1-454b-801e-64684b8654f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3fb9f-aa1f-430d-bb15-030f4ccb28d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67523c5e-2c19-4e4a-ab43-5bdd4d8d1cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
